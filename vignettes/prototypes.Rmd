---
title: "Permutation tests for adaptive designs"
subtitle: "Prototypes for experimental package features"
author: Florian Klinglmueller
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Vignette Title}
  \usepackage[utf8]{inputenc}
---

```{r}
library(magrittr)
library(plyr)
library(dplyr)
library(pander)
library(parallel)
library(bt88.03.704)
library(adaperm)
options(mc.cores=detectCores()-1)
```

# Adjusted p-values

Let $T_{(1)},...,T_{(|G|)}$ the ordered resampling test statistics of
the pre-planned test. Then the $p$-value of the test following an
adaptive interim analysis is given by $\tilde{p} = 1-\frac{r^*}{|\G|}$
where

$$
r^*=\min \{ r : \pe \leq A(T_{(r)})\}
$$


and $A(t)$ is defined as the conditional survival function of the
preplanned test given the first stage treatment assignments:

This is easily computed:

```{r}

sim_pvals <- function(B,delta,nulldist=rnorm,...){
    replicate(B,{
                  x1 <- nulldist(10,...)+rep(0:1,5)*delta
                  x2 <- nulldist(10,...)+rep(0:1,5)*delta
                  xE <- nulldist(6,...)+rep(0:1,3)*delta
                  pt <- t.test(c(x1,x2,xE)~rep(0:1,13),alternative='less',var.equal=T)$p.value
                  pdist  <- perm_dist(x1,x2,rep(0:1,5),rep(0:1,5),ranksum,B=10000)
                  pw <- wilcox.test(c(x1,x2,xE)~rep(0:1,13),alternative='less')$p.value
                  cdist <- adaperm:::cond_dist(x1,x2,rep(0:1,5),rep(0:1,5),ranksum,B=10000)
                  edist <- perm_dist(x2,xE,rep(0:1,5),rep(0:1,3),ranksum,B=10000)
                  t2 <- ranksum(c(x2,xE),rep(0:1,8))
                  c(sum(pdist>=quantile(cdist,sum(edist<t2)/length(edist)))/length(pdist),pt,pw)
              })
    }


pval_dist <- sim_pvals(1000,1,rnorm_cont,csd=10)
head(t(pval_dist))
cor(t(pval_dist)[,c(1,3)])
plot(t(pval_dist)[,c(1,3)])
par(mfrow=c(3,1))
plot(sort(pval_dist[1,]),seq(0,1,length.out=1000))
plot(sort(pval_dist[2,]),seq(0,1,length.out=1000))
plot(sort(pval_dist[3,]),seq(0,1,length.out=1000))

## lookin good!
rowSums(pval_dist<.05)/1000
hist(pval_dist[1,])



adaptive_permtest_ts <- function(x1,x2,xE,g1,g2,gE,stat,permutations=10000){
    pdist  <- perm_dist(x1,x2,g1,g2,stat,B=permutations)
    cdist <- adaperm:::cond_dist(x1,x2,g1,g2,stat,B=permutations)
    edist <- perm_dist(x2,xE,g2,gE,stat,B=permutations)
    t2 <- stat(c(x2,xE),c(g2,gE))
    pval <- sum(pdist>=quantile(cdist,sum(edist<t2)/length(edist)))/length(pdist)
    names(t2) <- deparse(substitute(stat))
    out <- list(statistic = t2,
                parameter = c('permutations'=permutations),
                p.value = pval,
                alternative = "one.sided",
                null.value = c("distribution of treated samples"='different from control samples'),
                data.name = paste(deparse(substitute(c(x1,x2,xE,g1,g2,gE))),collapse=', '),
                method = "Two-sample permutation test for adaptive designs")
    class(out) <- 'htest'
    out
}

str(t.test(rnorm(10)))
t.test(rnorm(10))
adaptive_permtest_ts(x1,x2,xE,g1,g2,gE,maritzm)
adaptive_permtest_ts(x1,x2,xE,g1,g2,gE,ranksum)

str(adaptive_permtest_ts(x1,x2,xE,g1,g2,gE,maritzm))



```

# Robus m-Tests based on maritz' m-estimator

Maritz et al. and Rosenbaum suggest the use of an $m$-estimator
(Huber) for use with a rerandomization test. In `adaperm` we have
implemented such a test statistic in the function `maritzm`.



```{r}
compare_adaptive_tests <- function(n1,n,rule,rdist,test_statistic,mpfr=F,resam=100,perms=100,...){
    x <- rdist(n,...)
    ne <- rule(x[1:n1])
    if(ne>n){
        x <- c(x,rdist(ne-n,...))
    } else {
        ne <- n
    }
    list(ne = ne,
         permtestT = adaptive_permtest_os(x,n1,n,ne,diffmean,perms=perms),
         invnormT = adaptive_invnorm_ttest_os(x,n1,n,ne),
         permtestW = adaptive_permtest_os(x,n1,n,ne,signedranks,perms=perms),
         invnormW = adaptive_invnorm_wilcoxtest_os(x,n1,n,ne),
         permtestM = adaptive_permtest_os(x,n1,n,ne,maritzm,perms=perms))
}

##' Compare operating characteristics for a number of one-sample tests
##'
##' \code{test_funs} needs to be a list of functions that take arguments \code{x,n1,n,ne,permutations} and return \code{TRUE} of \code{FALSE} depending on whether the corresponding decision rule rejects the null hypothesis or not.
##'
##' \code{rule} needs to be a function that takes as argument the vector of first stage observations and return an interger number, which (if larger than \code{n}) will be used as the new total sample size.
##' 
##' @title Compare one-sample test procedures
##' @param test_funs list of test functions (see Details)
##' @param n1 first stage sample size
##' @param n pre-planned total sample size
##' @param rule sample size reassessment rule (see Details)
##' @param rdist distribution from which to draw observations 
##' @param ... additional arguments to rdist
##' @return let's see
##' @author Florian Klinglmueller
compare_tests_onesample <- function(B,test_funs,n1,n,rule,rdist,...){
    run <- function(test_funs,n1,n,rule,rdist,...){
        x <- rdist(n,...)
        ne <- rule(x[1:n1])
        if(ne>n){
            x <- c(x,rdist(ne-n,...))
        } else {
            ne <- n
        }
        c(n1=n1,n=n,ne=ne,lapply(test_funs,do.call,list(x,n1,n,ne)))
    }
    results <- mclapply2(1:B,function(i) run(test_funs,n1,n,rule,rdist,...))
    results %>% bind_rows -> results
    eval(expr=parse(text=paste0("summarize(results,ASN=mean(ne),maxSN=max(ne),sdSN=sd(ne),",paste0(names(tests),"=","mean(",names(tests),")",collapse=','),")")))
}

tests <- list(
    permtestT = function(x,n1,n,ne) adaptive_permtest_os(x,n1,n,ne,diffmean,perms=10000),
    invnormT = function(x,n1,n,ne)  adaptive_invnorm_ttest_os(x,n1,n,ne),
    permtestW = function(x,n1,n,ne) adaptive_permtest_os(x,n1,n,ne,signedranks,perms=10000),
    invnormW = function(x,n1,n,ne) adaptive_invnorm_wilcoxtest_os(x,n1,n,ne),
    permtestM = function(x,n1,n,ne) adaptive_permtest_os(x,n1,n,ne,maritzm,perms=10000))

## Type I error 
compare_tests_onesample(1000,tests,10,20,cond_power_rule_norm,rnorm_cont,mean=0,sd=1.2,cshift=0,csd=3)
## Power
compare_tests_onesample(1000,tests,10,20,cond_power_rule_norm,rnorm_cont,mean=.5,sd=1,cshift=.5,csd=5)



delta <- 0
x1 <- rnorm(10)+rep(0:1,5)*delta
x2 <- rnorm(10)+rep(0:1,5)*delta
xE <- rnorm(6)+rep(0:1,3)*delta
g1 <- sign(x1);g2 <- sign(x2);gE <- sign(xE)
pt <- t.test(c(x1,x2,xE),alternative='less')$p.value
pdist  <- perm_dist(x1,x2,g1,g2,maritzm,B=10000,restricted=F)
pw <- wilcox.test(c(x1,x2,xE),alternative='less')$p.value
cdist <- adaperm:::cond_dist(x1,x2,g1,g2,maritzm,B=10000,restricted=F)
edist <- perm_dist(x2,xE,g2,gE,maritzm,B=10000,restricted=F)
t2 <- maritzm(c(x2,xE),rep(0:1,8))

c(sum(pdist>=quantile(cdist,sum(edist<t2)/length(edist)))/length(pdist),pt,pw)

adaptive_permtest_os(c(x1,x2,xE),10,20,26,maritzm,perms=10000)
permutation_CER(abs(x1),g1>0,abs(x2),maritzm,permutations=10000,restricted=F)
perm_test(abs(x2),abs(xE),g2,gE,maritzm,B=10000,restricted=F)


par(mfrow=c(3,1))
xl <- min(c(pdist,cdist,edist))
xu <- max(c(pdist,cdist,edist))
hist(pdist,xlim=c(xl,xu))
hist(cdist,xlim=c(xl,xu))
hist(edist,xlim=c(xl,xu))
abline(v=t2)



```


# Point estimates and confidence intervals

```{r}


adaptive_permtest_quick <- function(x1,x2,xE,g1,g2,gE,stat,permutations=10000){
    pdist  <- perm_dist(x1,x2,g1,g2,stat,B=permutations)
    cdist <- adaperm:::cond_dist(x1,x2,g1,g2,stat,B=permutations)
    edist <- perm_dist(x2,xE,g2,gE,stat,B=permutations)
    t2 <- stat(c(x2,xE),c(g2,gE))
    pval <- sum(pdist>=quantile(cdist,sum(edist<t2)/length(edist)))/length(pdist)
    pval
}


adaptive_permtest_ts <- function(x1,x2,xE,g1,g2,gE,stat,permutations=10000,conf_level=.05){
    pval <- adaptive_permtest_quick(x1,x2,xE,g1,g2,gE,stat,permutations=10000)
    err <- function(d,conf_level=.5){
        x1 <- x1-d*g1
        x2 <- x2-d*g2
        xE <- xE-d*gE
        adaptive_permtest_quick(x1,x2,xE,g1,g2,gE,stat,permutations=10000)-conf_level
    }
    t2 <- stat(c(x2,xE),c(g2,gE))
    names(t2) <- deparse(substitute(stat))
    ## rms
    guesstimate <- sqrt(mean(c(x1,x2,xE)^2))*sign(.5-pval)
    estimate <- c("constant additive effect"=uniroot(err,c(0,guesstimate))$root)
    conf.int <- c(uniroot(err,c(0,guesstimate*sign(conf_level-pval)),conf_level=conf_level)$root,Inf)
    attr(conf.int,"conf.level") <- 1-conf_level
    out <- list(statistic = t2,
                parameter = c('permutations'=permutations),
                p.value = pval,
                estimate = estimate,
                conf.int=conf.int,
                alternative = "one.sided",
                null.value = c("distribution of treated samples"='different from control samples'),
                data.name = paste(deparse(substitute(c(x1,x2,xE,g1,g2,gE))),collapse=', '),
                method = "Two-sample permutation test for adaptive designs")
    class(out) <- 'htest'
    out
}

g1 <- rep(0:1,5)
g2 <- rep(0:1,5)
gE <- rep(0:1,3)
x1 <- rnorm(10)+g1
x2 <- rnorm(10)+g2
xE <- rnorm(6)+gE

adaptive_permtest_ts(x1,x2,xE,g1,g2,gE,maritzm,perm=10000)
adaptive_permtest_ts(x1,x2,xE,g1,g2,gE,ranksum,perm=10000)
adaptive_permtest_ts(x1,x2,xE,g1,g2,gE,meandiff,perm=10000)

t.test(c(x1,x2,xE)~c(g1,g2,gE))
err(100)
uniroot(err,c(0,10))


```

# Group sequential procedures

```{r}
library(ldbounds)
## Obrien Fleming type boundaries

## inputs
alpha <- bounds(c(.5,1),alpha=.025)$exit.pr
g1 <- rep(0:1,10)
g2 <- rep(0:1,10)
x1 <- rnorm(20)
x2 <- rnorm(20)


## function
gsd2_permtest_ts <- function(x1,x2,g1,g2,alpha,stat,permutations){
    G <- omega(g1,g2,B= permutations)
    dist1 <- stat(x1,G[1:length(g1),])
    dist2 <- stat(c(x1,x2),G)
    to1 <- stat(x1,g1)
    to2 <- stat(c(x1,x2),c(g1,g2))
    B <- length(dist1)
    R1 <- rank(dist1,ties.method="min")>= B*(1-alpha[1])
    R2 <- rep(FALSE,B)
    R2[!R1] <- rank(dist2[!R1],ties.method="min")>= (B*(1-alpha[2]) + alpha[2]*sum(R1))
    ## Joint rejection probability
    list(jrp = sum(R1 | R2)/B,
         Ta1 = min(dist1[R1]),
         Ta2 = min(dist2[R2]),
         to1 = to1,
         to2 = to2,
         decision = (to1>=Ta1|to2>=Ta2))
}
             

gsd2_permtest_ts(x1,x2,g1,x2,alpha,meandiff,10^6)


```

# Adaptive group sequential procedures

```{r}
library(ldbounds)
## Obrien Fleming type boundaries

## inputs
alpha <- bounds(c(.5,1),alpha=.025)$exit.pr
g1 <- rep(0:1,10)
g2 <- rep(0:1,10)
gE <- rep(0:1,5)
x1 <- rnorm(20)
x2 <- rnorm(20)
xE <- rnorm(10)

permutations <- 10^4
stat <- maritzm


gsd2_permtest_ts <- function(x1,x2,xE,g1,g2,gE,alpha,stat,permutations){
    G <- omega(g1,g2,B= permutations)
    dist1 <- stat(x1,G[1:length(g1),])
    dist2 <- stat(c(x1,x2),G)
    to1 <- stat(x1,g1)
    to2 <- stat(c(x1,x2),c(g1,g2))
    B <- length(dist1)
    R1 <- rank(dist1,ties.method="min")>= B*(1-alpha[1])
    R2 <- rep(FALSE,B)
    R2[!R1] <- rank(dist2[!R1],ties.method="min")>= (B*(1-alpha[2]) + alpha[2]*sum(R1))
    Ta1 = min(dist1[R1])
    Ta2 = min(dist2[R2])
    jrp = sum(R1 | R2)/B
    cdist <- adaperm:::cond_dist(x1,x2,g1,g2,stat,permutations)
    ## reuse the alpha trimmings - this may be controversial if A=0 even though the test would control the error rate
    A <- sum(cdist>=Ta2)/length(cdist)+max(0,alpha[2]-jrp)
    GE <- omega(c(g2,gE),B=permutations)
    ## dirty trick to get non-finite cricital values if condiitonal error is exactly 0
    distE <- c(stat(c(x2,xE),GE),Inf)
    toE <- stat(c(x2,xE),c(g1,gE))
    ## The ranks are not changed by adding Inf, but now the vector won't be empty if A = 0
    TaE <- min(distE[rank(distE,ties.method='min') > (1-A)*(length(distE)-1)])
    ## Joint rejection probability
    
    list(a1 = sum(R1)/B,
         jrp = jrp,
         Ta1 = Ta1,
         TaE = TaE,
         to1 = to1,
         toE = toE,
         decision = (to1>=Ta1|toE>=TaE))
}
             

gsd2_permtest_ts(x1+g1,x2+g2,xE+gE,g1,g2,gE,alpha,meandiff,10^5)


```

# some subsetting

```{r,eval=F}

library(microbenchmark)



select1 <- function(G,sg1){
    tG <- t(G)     
    colnames(tG) <- paste0('V',1:ncol(tG))
    cond <- paste0('V',1:length(sg1),'==sg1[',1:length(sg1),']',collapse='&')
    tG[with(data.frame(tG),eval(parse(text=cond))),]
}

select2 <- function(G,sg1){
    tG <- t(G)     
    merge(matrix(sg1,nr=1),tG,by=1:length(sg1))
}

select3 <- function(G,sg1){
    tG <- as.data.table(t(G))
    sg1 <- as.data.table(as.data.frame(matrix(sg1,nr=1)))
    setkeyv(tG,paste0('V',1:length(sg1)))
    setkeyv(sg1,paste0('V',1:length(sg1)))
    tG[sg1,]                      
}

library(dplyr)

select4 <- function(G,sg1){
    inner_join(t(G) %>% as.data.frame %>% tbl_df,
               matrix(sg1,nr=1) %>% as.data.frame %>% tbl_df,by=paste0('V',1:length(sg1)))
}

select5 <- function(G,sg1){
    ## this may be dangerous and does not save time!!
    suppressMessages(inner_join(t(G) %>% as.data.frame %>% tbl_df,
                                matrix(sg1,nr=1) %>% as.data.frame %>% tbl_df))
    
}

n1=10
g1 <- rep(0:1,n1)
g2 <- rep(0:1,n1)
G <- omega(g1,g2,B=10^6)
dim(G)

sg1 <- sample(g1)

dim(select1(G,sg1))

microbenchmark(select1(G,sg1),
#               select2(G,sg1),
#               select3(G,sg1),
               select4(G,sg1),
               select5(G,sg1))

```

# Conditional error of the randomized test

Typically the randomized decision rule of the permutation test is
defined as

$$
\phi = \left\{\begin{array}{ll} 1 & \mbox{if} T_o > T_\alpha \\
\gamma & \mbox{if} T_o = T_\alpha \\
0 & otherwise
\end{array}\right.
$$

where $T_\alpha$ is the $\lceil (1-\alpha) * |G|\rceil$ order
statistic (using the minimum rank for ties) of the orbit $\{T(x): x
\in S(x)\}$ and $\gamma$ is defined as

$$
\frac{\alpha - P(T > T_\alpha)}{P(T=T_\alpha)}.
$$

The corresponding conditional error function is then

$$
A_R'=A' + \gamma P(T>T_\alpha| x_{|n_1} ).
$$


If either the $T_\alpha$ or the conditional permutation distribution
given the first stage observations are estimated using Monte-Carlo
permutation algorithm, it may be convenient to compute the conditional
error function of a slightly different randomized decision rule, which
flips a coin for all values of the test statistic falling within
$\epsilon$ of the critical value:  

$$
\phi_\epsilon = \gamma \mbof{if} T_o \in [T_\alpha - \epsilon,T_\alpha).
$$  

In this case $\gamma$ is given by

$$
\gamma = \frac{\alpha - P(T > T_\alpha)}{P(T \in [T_\alpha-\epsilon,T_\alpha))}
$$

which can be easily computed even for a Monte Carlo sample from the permutation space.
And the conditional error function is:

$$
A'_\eplsilon = A' + P(T \in [T_\alpha-\epsilon,T_\alpha)|x_{|n_1})\frac{\alpha - P(T > T_\alpha)}{P(T \in [T_\alpha-\epsilon,T_\alpha))}
$$  

For $\epsilon \rightarrow \infty$ one gets:

$$
A'_\infty = A' + (1-A')\frac{\alpha - P(T >T_\alpha)}{1-P(T > T_\alpha)}
$$

Alternatively one may use the conditional error function:

$$
A_t'= A' + (\alpha - P(T> T_\alpha))
$$  

which is also valid.

It can be easily seen that  

$$
E((1-A')\frac{\alpha - P(T >T_\alpha)}{1-P(T > T_\alpha)}) = (\alpha - P(T> T_\alpha)).
$$  

More interesting one sees that $\phi_t$ uses a constant randomization probability for all $T_o \leq T_\alpha$ whereas $\phi_t$ adds relatively more to the unrandomized conditional error for outcomes with larger conditional error rates, and less otherwise. Consequently using $A_t'$ makes more sense; using $A_R'$ makes probably most sense.

```{r}

```






# Variance of Ta

Most of the time we estimate Ta from a subset of all
permutations. Then we use that as a critical value when estimating the
conditional error. However, we estimate the conditional distribution
using another subset of permutations.

```{r,eval=F}

ta <- function(x,B,alpha,stat){
    pdist <- stat(x,omega(rep(0:1,length.out=length(x)),B=B))
    c(Ta=min(pdist[rank(pdist,ties='min') >= (1-alpha)*B]),Tap=min(pdist[rank(pdist,ties='min') >= (1-alpha+.01)*B]),Tam=min(pdist[rank(pdist,ties='min') >= (1-alpha-.01)*B]),min=min(pdist),max=max(pdist),q1=quantile(pdist,.25),q2=quantile(pdist,.75))
}

x <- rnorm(20)

ta3 <- replicate(1000,ta(x,10^3,.025,ranksum))
ta4 <- simplify2array(mclapply(1:1000,function(i) ta(x,10^4,.025,ranksum)))
ta5 <- simplify2array(mclapply(1:1000,function(i) ta(x,10^5,.025,ranksum)))
sd(ta3[1,])
mean(ta3[2,]-ta3[3,])
sd(ta4[1,])
mean(ta4[2,]-ta4[3,])
sd(ta5[1,])
mean(ta5[2,]-ta5[3,])

ta3 <- replicate(1000,ta(x,10^3,.025,maritzm))
ta4 <- simplify2array(mclapply(1:1000,function(i) ta(x,10^4,.025,maritzm)))
ta5 <- simplify2array(mclapply(1:1000,function(i) ta(x,10^5,.025,maritzm)))

sd(ta3[1,])
mean(ta3[2,]-ta3[3,])
sd(ta4[1,])
mean(ta4[2,]-ta4[3,])
sd(ta5[1,])
mean(ta5[2,]-ta5[3,])


ta3 <- replicate(1000,ta(x,10^3,.025,meandiff))
ta4 <- simplify2array(mclapply(1:1000,function(i) ta(x,10^4,.025,meandiff)))
ta5 <- simplify2array(mclapply(1:1000,function(i) ta(x,10^5,.025,meandiff)))

sd(ta3[1,])
mean(ta3[2,]-ta3[3,])
sd(ta4[1,])
mean(ta4[2,]-ta4[3,])
sd(ta5[1,])
mean(ta5[2,]-ta5[3,])



```

Conclusion, using $10^4$ ($10^5$) permutations to estimate $T_\alpha$ puts
about 10 (50) standard deviations between $T_{\alpha+1\%}$ and $T_{\alpha-1\%}$. 

## Is it faster to perform the test or to compute the p-value


```{r}
library(microbenchmark)

microbenchmark(adaptive_permtest_os(rnorm(20),10,15,20,maritzm),adaptive_permtest2_os(rnorm(20),10,15,20,maritzm))
## hm interestingly the p-value based test is slower by about 30%, I
## don't understand why though


```

