---
title: "Permutation tests for adaptive designs"
subtitle: "Refactoring of simulation studies"
author: Florian Klinglmueller
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Vignette Title}
  \usepackage[utf8]{inputenc}
output:
    pdf_document:
        keep_tex: true
    rmarkdown::html_vignette: default
---

```{r,echo=F,results='hide'}
knitr::opts_chunk$set(echo=FALSE,warning=FALSE,results='hide',fig.width=7,fig.height=7,message=FALSE)
```

```{r}
library(magrittr)
library(plyr)
library(dplyr)
library(reshape2)
library(ggplot2)
library(pander)
library(parallel)
library(bt88.03.704)
##devtools::install_github('floatofmath/adaperm')
library(adaperm)
options(mc.cores=detectCores()-1)

```

```{r, eval=F}
library(Rcpp)
sourceCpp("../src/combinations.cpp")
library(microbenchmark)
## about 90 fold time improvement
microbenchmark(all_combinations_cpp(16,8),gtools::combinations(16,8))
microbenchmark(all_reassignments_cpp(16,8),gtools::combinations(16,8))
null <- all_combinations(24,12)
## stress test
replicate(1000,{null <- all_combinations(20,10);NULL})

for(n in 1:24){
    for(k in 1:n){
        if(!all(t(all_combinations(n,k)) == gtools::combinations(n,k))){
            stop(paste('error at n =',n,'k =',k))
        }
    }
}
    
for(n in 1:24){
    for(k in 1:n){
        if(!all(all_reassignments_cpp(n,k) == adaperm:::all_reassignments(n,k))){
            stop(paste('error at n =',n,'k =',k))
        }
    }
}


```
# Simulation setup

## Tests under consideration

We look at three classes of tests:

1. **Inverse-normal combination tests**
   - Equal variance $t$-test 
   - Stage-wise Wilcoxon Rank-sum tests
2. **Permutation based conditional error tests**
   - Permutation test using $t$-statistic
   - Permutation test using Wilcoxon Rank-sum statistic


For the permutation test using the Wilcoxon Rank-sum statistic  we
additionally consider conditional error functions:

1. Based on the conservative preplanned permutation test
2. Based on the exact randomized preplanned permutation test,
   applying a conservative second stage test
3. Based on the exact randomized preplanned permutation test,
   applying a mid-$p$ value based second stage test


## Distributions

Naturally we need to consider the Normal distribution. Proschan
considers the following distributions:
  
- log-gamma distribution
- t-distribution
  
Gorman considers the following distributions:
  
- uniform
- t-distribution df=4
- lambda family 
- symmetric bimodal
- skewed bimodal


We consider:

1. normal distirbution
2. log-gamma distribution
3. t-distribution with 4 degrees of freedom
4. contaminated normal distribution with 10\% of observations sampled
   from a normal with three-times higher standard deviation

For all distributions we will study a scenario under the null, and two
scenarios under the alternative - one where the *Inverse-normal
combination $t$-test* has 60% power with a preplanned total per-group
sample size of 10, the other where it achieves 60% power with a
preplanned total per-group sample size of 100. 

## Sample size reassessment

In all cases we will apply a sample size reassessment rule based on
a conditional power rule that reestimates the sample size based on an
unblinded estimate of the variance. 


# Calibration of scenarios

## Test procedure

Tests where performed using functions implemented in `adaperm`. See
the code below for examples how to perform the different test an
example with 16 preplanned observations per group that is reassessed
to collect and additional 4 observations per group after the end of
the preplanned trial. 


```{r,echo=T}
x <- rnorm(20)
y <- rnorm(20)

adaptive_invnormtest_2s(x,y,n1=8,n=16,ne=20)
adaptive_invnorm_wilcoxtest_2s(x,y,n1=8,n=16,ne=20)
adaperm_DR(c(x,y),rep(0:1,each=20),n1=8,n=16,test=tstat)
adaperm_DR(c(x,y),rep(0:1,each=20),n1=8,n=16,test=ranksum)
adaperm_DR(c(x,y),rep(0:1,each=20),n1=8,n=16,test=ranksum,cer_type='randomized')
adaperm_DR(c(x,y),rep(0:1,each=20),n1=8,n=16,test=ranksum,cer_type='randomized',atest_type='midp')
    
```

## Distributions

The parameters of the distributions are calibrated using a manual
intersection search to give the desired operational
characteristics. That is 60\% and 80\% power using the inverse-normal
t-test with a preplanned sample size of 10 or 100 observations per
group. The parameters for the 80\% power scenario will be considered
the planning assumptions, for the 60\% power scenario will be used to
sample observations.

### Small sample scenarios

```{r,eval=F}
x <- rnorm(10)
y <- rnorm(10)
adaptive_invnormtest_2s(x,y,n1=5,n=10,ne=10)

## Assume m0 = 1 s0=.75 (power=80), true m0=1 s0=.9 (power=60)
power.t.test(10,1,.6)
mean(replicate(10000,{
sd <- .9
x <- rnorm(10,sd=sd)
y <- rnorm(10,m=1,sd=sd)
adaptive_invnormtest_2s(x,y,n1=5,n=10,ne=10)}))


x <- log(rgamma(10,1))
y <- log(rgamma(10,1))
adaptive_invnormtest_2s(x,y,n1=5,n=10,ne=10)


mean(replicate(10000,{a <- 1
                      ncp <- 1.35
                      x <- log(rgamma(10,a))
                      y <- log(rgamma(10,a))+ncp
                      adaptive_invnormtest_2s(x,y,n1=5,n=10,ne=10)}))

x <- rt(10,4)
y <- rt(10,4)
adaptive_invnormtest_2s(x,y,n1=5,n=10,ne=10)

mean(replicate(10000,{ncp <- 1.2
                      x <- rt(10,4)
                      y <- rt(10,4,ncp)
                      adaptive_invnormtest_2s(x,y,n1=5,n=10,ne=10)}))

mean(replicate(10000,{sd <- .8
                      x <- rnorm_cont(10,0,sd,cshift=0,csd=3*sd)
                      y <- rnorm_cont(10,1,sd,cshift=1,csd=3*sd)
                      adaptive_invnormtest_2s(x,y,n1=5,n=10,ne=10)}))


r_counts <- function(n,theta=1,df=2,lambda=NULL){
  if (is.null(lambda)) lambda=rchisq(n=n,df = df)
  lambda[lambda<.5]=.5
  lambda[lambda>10000]=10000
  y=rpois(n = n,lambda = lambda*theta)
  y
}

#This concludes the configuration of the small sample setup.
```



In all situations our planning assumption is normally distributed
observations with mean difference of $\delta_0=1$ and common standard deviation
of $\sigma_0=.6$ but simulate data from. Note that for these small
sample sizes there is quite a large difference in power between the
the inverse normal combination and the fixed sample t-test, the latter
has a power of 94% for $\sigma_0=.6$.

1. Normal distributions with mean-differences $\delta=1$ and $\sigma=.9$
2. Log-gamma distirbiutions with shape parameter $a=1$ shifted by
   $\delta=1.35$ in the treatment group
3. $t$-distributions with $4$ degrees of freedom and non-centrality
   parameter $1.2$ in the treatment group

in all cases the inverse normal combination test has a (simulated)
power of 60%.

### Large sample scenarios

```{r,eval=F}
x <- rnorm(100)
y <- rnorm(100)
adaptive_invnormtest_2s(x,y,n1=50,n=100,ne=100)

## Assume m0 = 1 s0=2.5 (power=80), true m0=1 s0=3.2 (power=60)
power.t.test(100,1,2.5)

mean(replicate(10000,{
sd <- 3.2
x <- rnorm(100,sd=sd)
y <- rnorm(100,m=1,sd=sd)
adaptive_invnormtest_2s(x,y,n1=50,n=100,ne=100)}))


x <- log(rgamma(100,1))
y <- log(rgamma(100,1))
adaptive_invnormtest_2s(x,y,n1=50,n=100,ne=100)


mean(replicate(10000,{a <- 1
                      ncp <- .4
                      x <- log(rgamma(100,a))
                      y <- log(rgamma(100,a))+ncp
                      adaptive_invnormtest_2s(x,y,n1=50,n=100,ne=100)}))

x <- rt(100,4)
y <- rt(100,4)
adaptive_invnormtest_2s(x,y,n1=50,n=100,ne=100)

mean(replicate(10000,{ncp <- .35
                      x <- rt(100,4)
                      y <- rt(100,4,ncp)
                      adaptive_invnormtest_2s(x,y,n1=50,n=100,ne=100)}))

adaptive_invnormtest_2s(x,y,n1=50,n=100,ne=100)

mean(replicate(10000,{sd <- 2.8
                      x <- rnorm_cont(100,0,sd,cshift=0,csd=3*sd)
                      y <- rnorm_cont(100,1,sd,cshift=1,csd=3*sd)
                      adaptive_invnormtest_2s(x,y,n1=50,n=100,ne=100)}))

```

```{r}
robust_pooled_variance <- function(x,y){
    (iqr(c(x-median(x),y-median(y)))/1.349)^2
}
cond_power_rule_t_ts <- function(x,y,delta=1,target=.9,alpha=0.025,maxN=length(x)*6,rob_var=T){
    n1 <- length(x)
    var <- ifelse(rob_var,
                  robust_pooled_variance(x,y),
                  pooled_variance(c(x,y),c(rep(0,length(x)),rep(1,length(y)))))
    nE <- 2*(qnorm(alpha,lower=F)+ qnorm(target))^2*var/(delta^2)
    min(maxN,nE)
}

```
```{r,eval=F}
mean(replicate(10000,{
                   sd <- 3.2
                   x <- rnorm(100,sd=sd)
                   y <- rnorm(100,m=1,sd=sd)
                   sqrt(robust_pooled_variance(x,y))}))

mean(replicate(10000,{
                   sd <- 3.2
                   x <- rnorm(100,sd=sd)
                   y <- rnorm(100,m=1,sd=sd)
                   sqrt(pooled_variance(c(x,y),rep(0:1,each=100)))}))

mean(replicate(10000,{ncp <- .35
                      x <- rt(100,4)*2.8
                      y <- rt(100,4,ncp)*2.8
                      sqrt(robust_pooled_variance(x,y))}))

mean(replicate(10000,{a <- 1
                      ncp <- .4
                      x <- log(rgamma(100,a))
                      y <- log(rgamma(100,a))+ncp
                      sqrt(robust_pooled_variance(x,y))}))

mean(replicate(10000,{ncp <- 1
                      x <- rnorm_cont(100,0,3.15,cshift=0,csd=3*3.15)
                      y <- rnorm_cont(100,ncp,3.15,cshift=ncp,csd=3*3.15)
                      sqrt(robust_pooled_variance(x,y))}))


mean(replicate(10000,{ncp <- 1
                      x <- rnorm_cont(100,0,3.15,cshift=0,csd=3*3.15)
                      y <- rnorm_cont(100,ncp,3.15,cshift=ncp,csd=3*3.15)
                      sqrt(pooled_variance(c(x,y),rep(0:1,each=100)))}))
                      

#This concludes the configuration of the large sample setup.
```


In all situations our planning assumption is normally distributed
observations with mean difference of $\delta_0=1$ and common standard deviation
of $\sigma_0=2.5$. For these larger sample sizes there is almost no
difference in power between the the inverse normal combination and the
fixed sample t-test. We will simulate data from:

1. Normal distributions with mean-differences $\delta=1$ and $\sigma=3.2$
2. Log-gamma distirbiutions with shape parameter $a=1$ shifted by
   $\delta=.4$ in the treatment group
3. $t$-distributions with $4$ degrees of freedom and non-centrality
   parameter $.35$ in the treatment group

in all cases the inverse normal combination test has a (simulated)
power of 60%.


```{r,eval=F}

distribution <- data_frame('Distribution'='Normal',
                           'Scenario'='Null',
                           'Sample-size'='Small',
                           'Data'=rnorm(10^6,0,.9))
distribution <- bind_rows(distribution,
                          data_frame('Distribution'='Normal',
                                     'Scenario'='Alternative',
                                     'Sample-size'='Small',
                                     'Data'=rnorm(10^6,1,.9)))
distribution <- bind_rows(distribution,
                          data_frame('Distribution'='Log-gamma',
                                     'Scenario'='Null',
                                     'Sample-size'='Small',
                                     'Data'=log(rgamma(10^6,1))))
distribution <- bind_rows(distribution,
                          data_frame('Distribution'='Log-gamma',
                                     'Scenario'='Alternative',
                                     'Sample-size'='Small',
                                     'Data'=log(rgamma(10^6,1))+1.35))
distribution <- bind_rows(distribution,
                          data_frame('Distribution'='t',
                                     'Scenario'='Null',
                                     'Sample-size'='Small',
                                     'Data'=rt(10^6,4,0)))
distribution <- bind_rows(distribution,
                          data_frame('Distribution'='t',
                                     'Scenario'='Alternative',
                                     'Sample-size'='Small',
                                     'Data'=rt(10^6,4,1.2)))
distribution <- bind_rows(distribution,
                          data_frame('Distribution'='Contaminated',
                                     'Scenario'='Null',
                                     'Sample-size'='Small',
                                     'Data'=rnorm_cont(10^6,mean=0,sd=.8,cprop=.1,cshift=0,csd=3*.8)))
distribution <- bind_rows(distribution,
                          data_frame('Distribution'='Contaminated',
                                     'Scenario'='Alternative',
                                     'Sample-size'='Small',
                                     'Data'=rnorm_cont(10^6,mean=1,sd=.8,cprop=.1,cshift=0,csd=3*.8)))


rec_relevel <- function(factor,levels){
    if(length(levels) == 1){
        relevel(factor,levels)
    } else {
        rec_relevel(relevel(factor,tail(levels,1)),head(levels,-1))
    }
}


distribution %<>% mutate(Scenario = relevel(factor(Scenario),'Null'))
distribution %<>% mutate(Distribution = rec_relevel(factor(Distribution),c('Normal','Log-gamma','Contaminated')))

distribution %>% group_by(Distribution,Scenario) %>% summarize(sd=sd(Data))

#distribution %<>% group_by(Distribution,Scenario) %>% mutate(Data = Data/sd(Data))

distribution_plot <- ggplot(distribution,aes(x=Data))+geom_line(aes(lty=Scenario),stat='density')+facet_wrap(~Distribution)+xlim(-7,10)+theme_minimal()

cairo_ps('distributions.eps')
print(distribution_plot)
dev.off()

```

# Simulation results


```{r}



run_scenario <- function(scenario){
    with(scenario,{
             xdist = function(N) {
                 switch(distribution,
                        'normal'=rnorm(N,m=0,sd=parameter),
                        'log-gamma'=log(rgamma(N,1)),
                        't4'=rt(N,4),
                        'cont'=rnorm_cont(N,m=0,sd=parameter,cshift=0,csd=3*parameter),
                        'count'=)
             }
             ydist = function(N) {
                 switch(distribution,
                        'normal'=rnorm(N,m=delta0,sd=parameter),
                        'log-gamma'=log(rgamma(N,1))+delta0*parameter,
                        't4'=rt(N,4,ncp=delta0*parameter),
                        'cont'=rnorm_cont(N,m=delta0,sd=parameter,cshift=0,csd=3*parameter),
                        'count'=)
             }
             x1 <- xdist(n1)
             y1 <- ydist(n1)
             delta <- c('normal'=1,
                        'log-gamma'=parameter,
                        't4'=parameter,
                        'cont'=1)[distribution]
             nE <- max(n,cond_power_rule_t_ts(x1,y1,delta=delta,target=.8))
             
             n_combs <- choose(2*n1,n1)*choose(2*(n-n1),(n-n1))
             exact  <- n_combs>10^5
                                        #  <- ifelse(n_combs>10000,'davison_hinkley','midp')
             ttpe <- 'non-randomized'
             cete <- 'non-randomized'
             x <- c(x1,xdist(nE-n1))
             y <- c(y1,ydist(nE-n1))
             data_frame(
                 exact=exact,
                 nE=nE,
#                 fixed_t=perm_test(c(x[1:n1],y[1:n1]),c(x[(n1+1):n],y[(n1+1):n]),rep(0:1,each=n1),rep(0:1,each=n1),tstat,10^5,restricted=T)<=0.025,
#                 fixed_w=perm_test(c(x[1:n1],y[1:n1]),c(x[(n1+1):n],y[(n1+1):n]),rep(0:1,each=n1),rep(0:1,each=n1),ranksum,10^5,restricted=T)<=0.025,
                 invnorm_t=adaptive_invnormtest_2s(x,y,n1=n1,n=n,ne=nE),
                 invnorm_w=adaptive_invnorm_wilcoxtest_2s(x,y,n1=n1,n=n,ne=nE),
                 adaperm_t=adaperm_DR(c(x,y),rep(0:1,each=nE),n1=n1,n=n,test=tstat,cer_type=cete,atest_type=ttpe,permutations=10^5),
                 adaperm_rw=adaperm_DR(c(x,y),rep(0:1,each=nE),n1=n1,n=n,test=ranksum,cer_type='randomized',atest_type='non-randomized',permutations=10^5),
                 adaperm_rwp=adaperm_DR(c(x,y),rep(0:1,each=nE),n1=n1,n=n,test=ranksum,cer_type='randomized',atest_type='midp',permutations=10^5),
                 adaperm_w=adaperm_DR(c(x,y),rep(0:1,each=nE),n1=n1,n=n,test=ranksum,cer_type='non-randomized',atest_type='non-randomized',permutations=10^5))
                                        #             adaperm_m=adaperm_DR(c(x,y),rep(0:1,each=nE),n1=n1,n=n,test=maritzm,atest_type='davison_hinkley'))
         })
}

simulate_scenario <- function(scenario,B){
    bind_rows(mclapply2(1:B,function(i) run_scenario(scenario))) %>%
        summarise(B=B,
                  pExact = mean(exact),
                  ASN = mean(nE),
                  sdSN = sd(nE),
                  MSN = max(nE),
#                  fixed_t = mean(fixed_t),
#                  fixed_w = mean(fixed_w),
                  invnorm_t=mean(invnorm_t),
                  invnorm_w=mean(invnorm_w),
                  adaperm_t=mean(adaperm_t),
                  adaperm_w=mean(adaperm_w),
                  adaperm_rw=mean(adaperm_rw),
                  adaperm_rwp=mean(adaperm_rwp))
#                  adaperm_m=mean(adaperm_m))
}
```

```{r,eval=F}

scenarios <- data_frame(n1 = rep(c(rep(5,4),rep(50,4)),2),
                n =  rep(c(rep(10,4),rep(100,4)),2),
                distribution = rep(rep(c('normal','log-gamma','t4','cont'),2),2),
                        parameter = rep(c(.9,1.35,1.2,.8,
                            3.2,.4,.35,2.8),2),
                delta0 = rep(0:1,each=8),
                sigma0 = rep(rep(c(.6,2.5),each=4),2))
                
## simulate_scenario(scenarios[2,],1000)
## simulate_scenario(scenarios[10,],5000)
## run_scenario(scenarios[2,])
## run_scenario(scenarios[10,])
out <- list() 
for(i in c(9:12)) out[[i]] <- bind_cols(scenarios[i,],simulate_scenario(scenarios[i,],B=50000))
sims <- bind_rows(out)
#for(i in 1:nrow(scenarios)) out[[i]] <- bind_cols(scenarios[i,],simulate_scenario(scenarios[i,],B=20000))

save(sims,file=vfile('~/adaperm_simresults/alt_simulations'))


```
For each scenario we simulated 50000 trials. For each trial we
computed the test decision of each test procedure where computed. Permutation
test used a maximum of 10^5 permutations for the computation of the
permutation distribution.

Simulations for the small sample scenarios took about a day using 30
cores of our simulation server. Simulations for the large sample
scenarios took up to one week using 30 cores of our simulation
server. This is due to the fact that for the small sample scenarios we
often do not need to perform computations for all 10^5 permutations
since the complete permutation space if often smaller.


```{r}
options(dplyr.width=120)
load('../data/null_simulations_node1_160323.Rd')
small_null_50k <- sims
load('../data/alt_simulations_node5_160323.Rd')
small_alt_50k <- sims
load('../data/null_simulations_large_node5_160327.Rd')
large_null_50k <- sims
load('../data/alt_simulations_large_node1_160330.Rd')
large_alt_50k <- sims

sims <- bind_rows(small_null_50k,
                  small_alt_50k,
                  large_null_50k,
                  large_alt_50k)

## pdf('simresults.pdf',height=4,width=7)
## sims %>% mutate(distribution = relevel(factor(distribution),'normal')) %>%
##     melt(.,id.vars=1:11,value.name='Power',variable.name='Test') %>% 
##         ggplot(aes(distribution,Power,fill=Test))+geom_bar(stat='identity',position='dodge')+facet_grid(delta0~n1,scales='free_y')
## dev.off()

sims %>% melt(.,id.vars=1:11,value.name='Power',variable.name='Test') %>%
    subset(delta0==0 & n1==5) %>% dcast(distribution~Test,value.var='Power') -> small_null_tab

sims %>% melt(.,id.vars=1:11,value.name='Power',variable.name='Test') %>%
    subset(delta0>0 & n1==5) %>% dcast(distribution~Test,value.var='Power') -> small_alt_tab

sims %>% melt(.,id.vars=1:11,value.name='Power',variable.name='Test') %>%
    subset(delta0==0 & n1==50) %>% dcast(distribution~Test,value.var='Power') -> large_null_tab
sims %>% melt(.,id.vars=1:11,value.name='Power',variable.name='Test') %>%
    subset(delta0>0 & n1==50) %>% dcast(distribution~Test,value.var='Power') -> large_alt_tab


```

## Type 1 error

```{r,results='markup'}
pander(small_null_tab,split.table=3000,digits=3,caption='Simulations of small sample scenarios under the null hypothesis')

```
```{r,results='markup'}
pander(large_null_tab,split.table=3000,digits=3,caption='Simulations of large sample scenarios under the null hypothesis')
```
We see that the Type I error rate is essentially controlled for all
scenarios. Small deviations from the nominal level are due to
simulation error. Even the test using the mid-$p$-value in the
second-stage test seem to control the Type I error rate.


## Power

```{r,results='markup'}
pander(small_alt_tab,split.table=3000,digits=3,caption='Simulations of small sample scenarios under the alternative hypothesis')
```

Looking at the power there are large differences between the small and
large sample scenarios. In the small sample scenarios the inverse
normal combination Wilcoxon test performs really bad, this is due to
the extreme discreteness of the conditional null distribution when the
test is applied stage-wise with only 5 observations per
group. Consequently applying an inverse normal combination t-test
outperforms the non-parametric test even for non-normal
observations. The conditional error based permutation test is 
affected by the small sample sizes to a much smaller degree and
outperforms the t-test for all but the normal scenario. The inverse
normal combination t-test and the conditional error permutation test
show similar performance in all secnarios. Using the conditional error
rate of a randomized pre-planned test adds on average 0.4 percentage
points to the power of the test procedure, performing the second stage
test using test based on the mid-$p$ value adds almost anothe
percentage point to the power. In comparison to the inverse normal
combination t-test we gain between 1 percentage point (normally
distributed observations) and 3 percentage points
(log-gamma, contaminated normal) in power.



```{r,results='markup'}
pander(large_alt_tab,split.table=3000,digits=3,caption='Simulations of large sample scenarios under the alternative hypothesis')
```

In the large sample scenario the inverse normal combination and
conditional error permutation test procedure perform very similar
under the alternative. Test procedures based on the $t$-Statistic are
outperformed by Wilcoxon type tests for all but the normal
scenario. The conditional error permutation test based on the wilcoxon
statistic outperform the combination tests by a small margin $.1$
percentage points. For the $t$-statistic it is the other way around. 
We conjecture that due to the asymptotic normality of the
$t$-statistic the inverse-normal combination is closer to the actual
conditional error rate of the preplanned test whereas the conditional
error permutation test uses the less efficient
"conditional-conditional" error rate. This difference may be reduced
using a sub-sampling approach to compute average the
'conditional-coniditional'-error rate over many subsamples - with the
size of the preplanned second stage - of an extended second stage,
which asymptotically (though requiring the sample size extension going
to inifinity) converges to the unconditional conditional error rate.


```{r simulations for pauly,eval=F}
set.seed(10514)
library(flip)
B <- 1000
n <- 20
X0 <- matrix(rt(n*B,df=2),n)
X <- matrix(rt(n*B,df=2),n)+1


ttests0 <- apply(X0,2,t.test,alternative='greater')
ttests0_pvalues <- sapply(ttests0,`[[`,'p.value')
ttests <- apply(X,2,t.test,alternative='greater')
ttests_pvalues <- sapply(ttests,`[[`,'p.value')


comb10 <- apply(X0[1:(n/2),],2,t.test,alternative='greater')
comb10_pvalues <- sapply(comb10,`[[`,'p.value')
comb20 <- apply(X0[(n/2+1):n,],2,t.test,alternative='greater')
comb20_pvalues <- sapply(comb20,`[[`,'p.value')
comb0_pvalues <- pnorm(sqrt(.5)*(qnorm(comb10_pvalues,lower=F)+qnorm(comb20_pvalues,lower=F)),lower=F)

comb1 <- apply(X[1:(n/2),],2,t.test,alternative='greater')
comb1_pvalues <- sapply(comb1,`[[`,'p.value')
comb2 <- apply(X[(n/2+1):n,],2,t.test,alternative='greater')
comb2_pvalues <- sapply(comb2,`[[`,'p.value')
comb_pvalues <- pnorm(sqrt(.5)*(qnorm(comb1_pvalues,lower=F)+qnorm(comb2_pvalues,lower=F)),lower=F)

permtests0 <- flip(X0,statTest='t',tail=1,perms=10000)
permtests0_pvalues <- permtests0@res$`p-value`
permtests <- flip(X,statTest='t',tail=1,perms=10000)
permtests_pvalues <- permtests@res$`p-value`

wilctests0 <- flip(X0,statTest='Wilcoxon',tail=1,perms=10000)
wilctests0_pvalues <- wilctests0@res$`p-value`
wilctests <- flip(X,statTest='Wilcoxon',tail=1,perms=10000)
wilctests_pvalues <- wilctests@res$`p-value`

mean(ttests0_pvalues<=.05)
mean(permtests0_pvalues<=.05)
mean(comb0_pvalues<=.05)
mean(wilctests0_pvalues<=.05)

mean(ttests_pvalues<=.05)
mean(permtests_pvalues<=.05)
mean(comb_pvalues<=.05)
mean(wilctests_pvalues<=.05)

```
